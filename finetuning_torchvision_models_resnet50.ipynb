{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# since I added the weights file of the pretrained Resnet50 model to this kernel, there're two folders in the ../input\n# ../input/plant-seedlings-classification and ../input/ResNet-50\nprint(os.listdir(\"../input\"), os.listdir(\"../input/plant-seedlings-classification/train\"))\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89a37d9f27ec4b0b4a54bc697824e3aa4f17e64d"
      },
      "cell_type": "code",
      "source": "# how many files in each category\ntraining_base_dir = \"../input/plant-seedlings-classification/train\"\nfile_nums = []\ncategory_files = {}\nfor directory in os.listdir(training_base_dir):\n    category_files[directory] = []\n    cate_dir = os.path.join(training_base_dir, directory)\n    file_num = 0\n    for file_name in os.listdir(cate_dir):\n        full_file_name = os.path.join(cate_dir, file_name)\n        category_files[directory].append(full_file_name)\n        file_num += 1\n    print(cate_dir, file_num)\n    file_nums.append(file_num)\n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05282a84cc8f2d061bec7f1052c2bb68ccaeaf25"
      },
      "cell_type": "code",
      "source": "# create folders for training and validation data\nbase_dir = \"./\"\ncategories = os.listdir(\"../input/plant-seedlings-classification/train\")\n# I initially wanted to create 4 training datasets and 4 validation datasets and use theses datasets to train different models.\n# Since there's only 5 GB disk I can use, only one training dataset and one validation dataset are created.\ndatasets_num = 1\nfor idx in range(1, datasets_num + 1):\n    train_val_str = [data_type + str(idx) for data_type in [\"train\", \"val\"]]\n    for data_type in train_val_str:\n        tmp_path0 = os.path.join(base_dir, data_type)\n        try:\n            os.mkdir(tmp_path0)\n        except (FileExistsError, FileNotFoundError):\n            print(\"raise an error when creating {}\".format(tmp_path))\n            continue        \n        for category in categories:\n            tmp_path1 = os.path.join(tmp_path0, category)\n            try:\n                os.mkdir(tmp_path1)\n            except (FileExistsError, FileNotFoundError):\n                print(\"raise an error when creating {}\".format(tmp_path))\n                continue",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a3919a8e0bd044d60b3f57ee9b84e2351b9ed2c"
      },
      "cell_type": "code",
      "source": "# sample files and copy these files to training and validation dataset folders\nfrom shutil import copy\nfrom random import sample, seed\nfrom pdb import set_trace\nseed()\nfor i in range(1, datasets_num + 1):\n    for _, category in enumerate(category_files.keys()):\n        l = len(category_files[category])\n        train_data_num = int(l * 0.9)\n        valid_data_num = l - train_data_num\n        files2copy = sample(category_files[category], l)\n        train_dest = os.path.join(base_dir, \"train{}\".format(i), category)\n        valid_dest = os.path.join(base_dir, \"val{}\".format(i), category)\n        for j in range(train_data_num):\n            copy(files2copy[j], train_dest)\n        for j in range(train_data_num, l):\n            copy(files2copy[j], valid_dest)\n            ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88737f691f2cfb53f4fad7c2e256b4bd5f89a747"
      },
      "cell_type": "code",
      "source": "# create a folder to store the weights of pretrained models\nfrom os.path import join, exists\nfrom os import makedirs\ncache_dir = join('/tmp', '.torch')\nif not exists(cache_dir):\n    print(\"creating {}\".format(cache_dir))\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    print(\"creating {}\".format(models_dir))\n    makedirs(models_dir)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cc44b22fb8f8a3027836567f3699cd4bd122b5c"
      },
      "cell_type": "code",
      "source": "# !cp ../input/resnet101/resnet101.pth /tmp/.torch/models/resnet101-5d3b4d8f.pth\n# we need to find the weights of the pretrained model in https://www.kaggle.com/pytorch and add the weiths to our kernel.\n# since pytorch finds the weights in /tmp/.torch/models, we need to copy the weights file to the folder and rename it.\n!cp ../input/resnet50/resnet50.pth /tmp/.torch/models/resnet50-19c8e357.pth",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46f734202a5a407d28c6e78aa8359caec3e60ecb"
      },
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision as tv\nfrom torchvision import transforms\n# define transforms\nimg_size = (224, 224)\ntrain_transforms = transforms.Compose([transforms.Resize(img_size), transforms.RandomHorizontalFlip(0.5), \n                                   transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\nvalid_transforms = transforms.Compose([transforms.Resize(img_size), \n                                   transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n# define data loader\nbatch_size = 64\ntrain_dataset = tv.datasets.ImageFolder(\"train1\", train_transforms)\nvalid_dataset = tv.datasets.ImageFolder(\"val1\", valid_transforms)\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n\n# define model\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\nmyModel = tv.models.resnet50(pretrained=True)\nset_parameter_requires_grad(myModel, True)\nnum_ftrs = myModel.fc.in_features\nnum_classes = len(category_files.keys())\n# three layers are added to the top of the pretrained model\nmyModel.fc = nn.Sequential(*[nn.Linear(num_ftrs, 1024), nn.Dropout(0.25), nn.Linear(1024, num_classes)])\n\n# define optimizer and loss\noptimizer = optim.SGD(myModel.parameters(), lr=0.001, momentum=0.9)\ncriterion = nn.CrossEntropyLoss()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca4e84878a23ed8e457cc385e57522542712115d"
      },
      "cell_type": "code",
      "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmyModel = myModel.to(device)\nepochs = 200\nbest_val_loss = 100\nfor i in range(epochs):\n    train_loss = 0\n    train_corrects = 0\n    myModel.train()\n    for inputs, labels in train_data_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        predicts = myModel(inputs)\n        loss = criterion(predicts, labels)\n        loss.backward()\n        optimizer.step()\n        _, preds = torch.max(predicts, 1)\n        train_loss += loss.item() * inputs.size(0)\n        train_corrects += torch.sum(preds == labels.data)\n    val_loss = 0\n    val_corrects = 0\n    myModel.eval()\n    for inputs, labels in valid_data_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        predicts = myModel(inputs)\n        loss = criterion(predicts, labels)\n        _, preds = torch.max(predicts, 1)\n        val_loss += loss.item() * inputs.size(0)\n        val_corrects += torch.sum(preds == labels.data)        \n    print(\"epoch: {}, train loss: {}, train accu: {}, val loss: {}, val accu: {}\".format(i, \n        train_loss / len(train_data_loader.dataset), train_corrects.double() / len(train_data_loader.dataset), \n        val_loss / len(valid_data_loader.dataset), val_corrects.double() / len(valid_data_loader.dataset)))\n    if val_loss / len(valid_data_loader.dataset) < best_val_loss:\n        torch.save(myModel.state_dict(), \"myModel.pkl\")\n        best_val_loss = val_loss / len(valid_data_loader.dataset)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}